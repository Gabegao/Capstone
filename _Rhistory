qplot(factor(am), mpg, data=mtcars, geom="boxplot")
fit<-lm(mpg~am, data=mtcars)
summary(fit)
?mtcars
fit2<-lm(mpg~., data=mtcars)
summary(fit2)
table(mtcars)
qplot(factor(am), mpg, data=mtcars, geom="boxplot")
library(datasets)
data(mtcars)
library(ggplot2)
qplot(factor(am), mpg, data=mtcars, geom="boxplot")
fit1<-lm(mpg~am, data=mtcars)
summary(fit1)
e<-resid(fit1)
fit2<-lm(mpg~., data=mtcars)
summary(fit2)
e<-resid(fit2)
fit2<-lm(mpg~am + hp, data=mtcars)
summary(fit2)
e<-resid(fit2)
plot(fit1)
plot(fit1)
library(datasets)
data(mtcars)
fit1<-lm(mpg~am, data=mtcars)
fit2<-lm(mpg~am + hp, data=mtcars)
plot(predict(fit), resid(fit), pch='.')
plot(predict(fit1), resid(fit1), pch='.')
plot(predict(fit1), resid(fit1), pch=19)
plot(predict(fit2), resid(fit2), pch=19)
?mtcars
qplot(factor(am), mpg, data=mtcars, geom="boxplot")
library(ggplot2)
qplot(factor(am), mpg, data=mtcars, geom="boxplot")
fit1<-lm(mpg~am, data=mtcars)
summary(fit1)
plot(predict(fit1), resid(fit1), pch=19)
automatic<-mtcars$mpg[mtcars$am == 0]
manual<-mtcars$mpg[mtcars$am == 1]
t.test(automatic, manual, paired=FALSE, var.equal = FALSE)
fit1<-lm(mpg~am, data=mtcars)
summary(fit1)
plot(predict(fit1), resid(fit1), pch=19)
fit2<-lm(mpg~am + hp, data=mtcars)
summary(fit2)
plot(predict(fit2), resid(fit2), pch=19)
plot(fit2)
?mtcars
fit3<-lm(mpg~am + hp + cyl, data=mtcars)
summary(fit3)
fit3<-lm(mpg~am + hp + wt, data=mtcars)
summary(fit3)
fit3<-lm(mpg~., data=mtcars)
summary(fit3)
fit2<-lm(mpg~am * hp, data=mtcars)
summary(fit2)
fit3<-lm(mpg~., data=mtcars)
summary(fit3)
annova(fit1, fit2, fit3)
anova(fit1, fit2, fit3)
fit1<-lm(mpg~am, data=mtcars)
fit2<-lm(mpg~am + hp, data=mtcars)
fit3<-lm(mpg~., data=mtcars)
anova(fit1, fit2, fit3)
summary(fit2)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
cutCStrength<- cut2(training$CompressiveStrength, g=3)
table(cutCStrength)
p1<- qplot(cutCStrength, CompressiveStrength, data=training, fill=cutCStrength, geom=c("boxplot"))
p1
install.packages("digest")
p1
p1<- qplot(cutCStrength, Age, data=training, fill=cutCStrength, geom=c("boxplot"))
p1
p1<- qplot(cutCStrength, FlyAsh, data=training, fill=cutCStrength, geom=c("boxplot"))
p1
p1<- qplot(cutCStrength, Age, data=training, fill=cutCStrength, geom=c("boxplot"))
p1
p1<- qplot(cutCStrength, FlyAsh, data=training, fill=cutCStrength, geom=c("boxplot"))
p1
p1<- qplot(cutCStrength, CompressiveStrength, data=training, fill=cutCStrength, geom=c("boxplot"))
p1
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
featureplot(x=training[,c("age", "Cement", "BlastFurnaceSlag", "FlyAsh", "Water"), y=training$CompressiveStrength, plot="pairs"])
featurePlot(x=training[,c("age", "Cement", "BlastFurnaceSlag", "FlyAsh", "Water"), y=training$CompressiveStrength, plot="pairs"])
featurePlot(x=training[,c("age", "Cement", "BlastFurnaceSlag", "FlyAsh", "Water")], y=training$CompressiveStrength, plot="pairs")
featurePlot(x=training[,c("Age", "Cement", "BlastFurnaceSlag", "FlyAsh", "Water")], y=training$CompressiveStrength, plot="pairs")
hist(mixtures$Superplasticizer)
hist(log(mixtures$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
?preProcess
names(training)
ptraining<- preProcess(training[,c(58:69)])
ptraining
?prcomp
prComp<-prcomp(ptraining)
M<- abs(cor(ptraining))
M<- abs(cor(ptraining[,-58]))
class(ptraining)
ptraining<-data.frame(ptraining)
hist(mixtures$Superplasticizer)
hist(log(mixtures$Superplasticizer))
hist(log(mixtures$Superplasticizer+1))
ptraining<- preProcess(training[,c(58:69)])
prComp<- prcomp(ptraining)
prComp<- prcomp(training[,c(58:69)])
prComp
preProc <- preProcess(training[,c(58:69)], method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation
install.packages('AppliedPredictiveModeling')
install.packages('caret')
install.packages.version('caret','v6.0.47')
iinstall.packages('ElemStatLearn')
install.packages('ElemStatLearn')
install.packages('pgmm')
install.packages('rpart')
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
data(segmentationOriginal)
inTrain<- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training<- segmentationOriginal[inTrain,]
testing<- segmentationOriginal[-inTrain,]
dim(training)
dim(testing)
?train
training<- segmentationOriginal[Case == "Train",]
training<- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing<- segmentationOriginal[segmentationOriginal$Case == "Test",]
?set.seed
set.seed(125)
modFit<- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
modFit<- train(Class ~ ., method="rpart", data=training)
install.packages('e1071')
modFit<- train(Class ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
install.packages('rattle')
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training<- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing<- segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modFit<- train(Class ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(pgmm)
data(olive)
olive=olive[,-1]
inTrain<-createDataPartition(y=olive$Area, p=0.75, list=FALSE)
training<-olive[inTrain,]
testing<-olive[-inTrain,]
dim(training)
dim(testing)
modFit1<- train(Area~., method='rpart', data=training)
print(modFit1$finalModel, uniform=TRUE)
print(modFit1$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit1$finalModel, use.n=TRUE, all=TRUE, cex=.8)
print(modFit1$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit1$finalModel, use.n=TRUE, all=TRUE, cex=.8)
print(modFit1$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit1$finalModel, use.n=TRUE, all=TRUE, cex=.8)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit1, newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
?train
set.seed(13234)
modFit2<- train(chd~age+alcohol+obesity+tobacco+typea+ldl, method="glm", family="binomial", data=trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA, predict(modFit2, testSA))
missClass(trainSA, predict(modFit2, trainSA))
missClass(testSA$chd, predict(modFit2, testSA))
missClass(trainSA$chd, predict(modFit2, trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.test)
head(vowel.train)
vowel.test$y<- as.factor(vowel.test$y)
vowel.train$y<- as.factor(vowel.train$y)
set.seed(33833)
modFit3<- train(y~., data=vowel.train, method="rf", prox=TRUE)
modFit3<- train(y~., data=vowel.train, method="rf", prox=TRUE)
modFit3
?varImp
varImp(modFit3)
modFit3<- train(y~., data=vowel.train, method="rf", importance=FALSE)
varImp(modFit3)
set.seed(33833)
modFit3<- train(y~., data=vowel.train, method="rf", importance=FALSE)
varImp(modFit3)
install.packages("ElemStatLearn")
install.packages("pgmm")
install.packages("gbm")
install.packages("lubridate")
install.packages("forecast")
-code--code-library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
-/code--/code-
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y<-as.factor(vowel.train$y)
vowel.test$y<-as.factor(vowel.test$y)
set.seed(33833)
library(caret)
mod1<- train(y~., method="rf", data=vowel.train)
mod2<- train(y~., method="gbm", data=vowel.train)
fcast1<-predict(mod1, vowel.test)
fcast2<-predict(mod2, vowel.test)
accuracy(fcast1, vowel.test$y)
?accuracy
library(forecast)
accuracy(fcast1, vowel.test$y)
table(fcast1, fcast2)
confusionMatrix(vowel.test$y, fcast1.result)$overall['Accuracy']
confusionMatrix(vowel.test$y, fcast1)$overall['Accuracy']
confusionMatrix(vowel.test$y, fcast2)$overall['Accuracy']
idx_agreed <- (fcast1 == fcast2)
confusionMatrix(vowel.test$y[idx_agreed], fcast1[idx_agreed])$overall['Accuracy']
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
inTrain<-createDataPartition(adData$diagnosis, p=3/4)[[1]]
training<-adData[inTrain,]
testing<-adData[-inTrain,]
adData<-data.frame(diagnosis, predictors)
inTrain<-createDataPartition(adData$diagnosis, p=3/4)[[1]]
training<-adData[inTrain,]
testing<-adData[-inTrain,]
set.seed(62433)
mod1<- train(diagnosis~., method="rf", data=training)
mod2<- train(diagnosis~., method="gbm", data=training)
mod3<- train(diagnosis~., method="lda", data=training)
pred1<-predict(mod1, testing)
pred2<-predict(mod2, testing)
pred3<-predict(mod3, testing)
pred<- data.frame(pred1, pred2, pred3, diag=testing$diagnosis)
combModFit<- train(dia~., method="rf", data=pred)
combPred<- predict(combModFit, pred)
combModFit<- train(diag~., method="rf", data=pred)
combPred<- predict(combModFit, pred)
confusionMatrix(testing$diagnosis, combPred)$overall['Accuracy']
confusionMatrix(testing$diagnosis, pred1)$overall['Accuracy']
confusionMatrix(testing$diagnosis, pred2)$overall['Accuracy']
confusionMatrix(testing$diagnosis, pred3)$overall['Accuracy']
set.seed(3523)
data(concrete)
inTrain<- createDataPartition(concrete$CompressiveStrength, p=3/4)[[1]]
training<- concrete[inTrain,]
testing<- concrete[-inTrain,]
set.seed(233)
fit<-train(CompressiveStrength~., data=training, method="lasso")
set.seed(233)
fit<-train(CompressiveStrength~., data=training, method="lasso")
plot.enet(fit$finalModel, xvar="penalty", use.color=TRUE)
library(lubridate)
dat<-read.csv("~/Desktop/gaData.csv")
getwd()
dat<-read.csv("~/gaData.csv")
training<-dat[year(dat$date) < 2012,]
testing<-dat[year(dat$date) > 2011,]
tstrain<-ts(training$visitsTumblr)
library(forecast)
?bats
mod <- bats(tstrain)
fcast <- forecast.bats(mod, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
count <- 0
for (i in 1:nrow(testing)) {
if (testing$visitsTumblr[i] > fcast$lower[i]) {
if(testing$visitsTumblr[i] < fcast$upper[i]) {
count <- count + 1}
}
}
count/nrow(testing)
set.seed(3523)
data(concrete)
inTrain<- createDataPartition(concrete$CompressiveStrength, p=3/4)[[1]]
training<- concrete[inTrain,]
testing<- concrete[-inTrain,]
set.seed(325)
library(e1071)
install.packages("shiny")
library(shiny)
shiny::runApp('C:/Users/i55336/Downloads/ShinyApp')
shiny::runApp('C:/Users/i55336/Downloads/Coursera/Coursera-SwiftKey/DataProductProject')
shiny::runApp('C:/Users/i55336/Downloads/Coursera/Coursera-SwiftKey/ShinyApp')
setwd("C:/Users/i55336/Downloads/Coursera/Coursera-SwiftKey/final/en_US")
load('./onegram.Rdata')
load('./twogram.Rdata')
load('./threegram.Rdata')
onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
threegram_f <- sort(colSums(as.matrix(threegram)), decreasing=TRUE)
onegram<- removeSparseTerms(onegram, 0.99)
twogram<- removeSparseTerms(twogram, 0.99)
threegram<- removeSparseTerms(threegram, 0.99)
library('tm')
library('stringi')
library('RWeka')
onegram<- removeSparseTerms(onegram, 0.99)
twogram<- removeSparseTerms(twogram, 0.99)
threegram<- removeSparseTerms(threegram, 0.99)
onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
threegram_f <- sort(colSums(as.matrix(threegram)), decreasing=TRUE)
load('./Cleandataset.Rdata')
library('tm')
library('stringi')
library('RWeka')
set.seed(1234)
index<- sample(1:length(dataset), length(dataset)*0.4)
training<- dataset[index]
test<- dataset[-index]
save(training, test, file='./datasetsplit.Rdata')
rm('dataset','test','index')
mycorpus<- Corpus(VectorSource(training))
## Remove Punctuation
mycorpus<- tm_map(mycorpus, removePunctuation)
## Remove numbers
mycorpus<- tm_map(mycorpus, removeNumbers)
## Remove profanities (profanity list found from web)
profanity_file<- url("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt")
profanities <- readLines(profanity_file)
close(profanity_file)
mycorpus<- tm_map(mycorpus, removeWords, profanities)
## Stripe white space
mycorpus<- tm_map(mycorpus, stripWhitespace)
## Convert to lower case
mycorpus<- tm_map(mycorpus, content_transformer(tolower))
save(mycorpus, file = './mycorpus_final_042616.Rdata')
rm('training','profanities','profanity_file')
onegram_t<- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
onegram<- DocumentTermMatrix(mycorpus, control=list(tokenize=onegram_t))
onegram <- removeSparseTerms(onegram, 0.99)
onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
save(onegram, file='./onegram_042616.Rdata')
## two-gram tokenizer, remove sparse terms in the end
twogram_t<- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogram<- DocumentTermMatrix(mycorpus, control=list(tokenize=twogram_t))
twogram<- removeSparseTerms(twogram, 0.99)
save(twogram, file='./twogram_042616.Rdata')
## three-gram tokenizer, remove sparse terms in the end
threegram_t<- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
threegram<- DocumentTermMatrix(mycorpus, control=list(tokenize=threegram_t))
threegram<- removeSparseTerms(threegram, 0.99)
save(threegram, file='./threegram_042616.Rdata')
## four-gram tokenizer, remove sparse terms in the end
fourgram_t<- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
fourgram<- DocumentTermMatrix(mycorpus, control=list(tokenize=fourgram_t))
save(fourgram, file='./fourgram_042616.Rdata')
rm('onegram_f','onegram','twogram','threegram')
fourgram<- DocumentTermMatrix(mycorpus, control=list(tokenize=fourgram_t))
#fourgram<- removeSparseTerms(fourgram, 0.99)
save(fourgram, file='./fourgram_042616.Rdata')
## five-gram tokenizer, remove sparse terms in the end
#fivegram_t<- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
fivegram<- DocumentTermMatrix(mycorpus, control=list(tokenize=fivegram_t))
#fivegram<- removeSparseTerms(fivegram, 0.99)
save(fivegram, file='./fivegram_042616.Rdata')
rm('fourgram')
fivegram_t<- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
fivegram<- DocumentTermMatrix(mycorpus, control=list(tokenize=fivegram_t))
#fivegram<- removeSparseTerms(fivegram, 0.99)
save(fivegram, file='./fivegram_042616.Rdata')
load('./fourgram_042616.Rdata')
load('./fivegram_042616.Rdata')
fourgram_f<- sort(colSums(as.matrix(fourgram)), decreasing=TRUE)
fivegram_f<- sort(colSums(as.matrix(fivegram)), decreasing=TRUE)
fourgram<- removeSparseTerms(fourgram, 0.999)
fivegram<- removeSparseTerms(fivegram, 0.999)
fourgram_f<- sort(colSums(as.matrix(fourgram)), decreasing=TRUE)
fivegram_f<- sort(colSums(as.matrix(fivegram)), decreasing=TRUE)
fourgram_f
load('./onegram_042616.Rdata')
load('./twogram_042616.Rdata')
load('./threegram_042616.Rdata')
onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
threegram_f<- sort(colSums(as.matrix(threegram)), decreasing=TRUE)
save(onegram_f, twogram_f, threegram_f, fourgram_f, file='./frequency1234.Rdata')
load('./mycorpus_final_042616.Rdata')
# load('./onegram.Rdata')
# load('./twogram.Rdata')
# load('./threegram.Rdata')
## one-gram tokenizer, remove sparse terms in the end
onegram_t<- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
onegram<- DocumentTermMatrix(mycorpus, control=list(tokenize=onegram_t))
onegram <- removeSparseTerms(onegram, 0.999)
onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
save(onegram, file='./onegram_042616.Rdata')
## two-gram tokenizer, remove sparse terms in the end
twogram_t<- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogram<- DocumentTermMatrix(mycorpus, control=list(tokenize=twogram_t))
twogram<- removeSparseTerms(twogram, 0.999)
save(twogram, file='./twogram_042616.Rdata')
## three-gram tokenizer, remove sparse terms in the end
threegram_t<- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
threegram<- DocumentTermMatrix(mycorpus, control=list(tokenize=threegram_t))
threegram<- removeSparseTerms(threegram, 0.999)
save(threegram, file='./threegram_042616.Rdata')
rm('mycorpus')
load('./fourgram_042616.Rdata')
onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
onegram <- removeSparseTerms(onegram, 0.99)
twogram <- removeSparseTerms(onegram, 0.99)
onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
threegram_f<- sort(colSums(as.matrix(threegram)), decreasing=TRUE)
fourgram_f<- sort(colSums(as.matrix(fourgram)), decreasing=TRUE)
save(onegram_f, twogram_f, threegram_f, fourgram_f, file='./frequency1234.Rdata')
firstname<-sapply(strsplit(names(twogram_f), ' '), function(a) a[1])
secname<-sapply(strsplit(names(twogram_f), ' '), function(a) a[2])
firsttriname<-sapply(strsplit(names(threegram_f), ' '), function(a) a[1])
sectriname<-sapply(strsplit(names(threegram_f), ' '), function(a) a[2])
thirtriname<-sapply(strsplit(names(threegram_f), ' '), function(a) a[3])
unigram<-data.frame(uni=names(onegram_f), freq=onegram_f, stringsAsFactors = F)
bigram<- data.frame(bi=names(twogram_f), freq=twogram_f, unigram=firstname, name=secname)
trigram<- data.frame(tri=names(threegram_f), freq=threegram_f, bigram=paste(firsttriname, sectriname), name=thirtriname)
save(unigram, bigram, trigram,  file='./ngram123.Rdata')
load('frequency1234.rdata')
load('ngram123.rdata')
load('./twogram_042616.Rdata')
load('./threegram_042616.Rdata')
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
twogram <- removeSparseTerms(onegram, 0.999)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
twogram <- removeSparseTerms(onegram, 0.99)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
rm('threegram')
twogram <- removeSparseTerms(twogram, 0.999)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
twogram <- removeSparseTerms(twogram, 0.99)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
load('./mycorpus_final_042616.Rdata')
# load('./onegram.Rdata')
# load('./twogram.Rdata')
# load('./threegram.Rdata')
## one-gram tokenizer, remove sparse terms in the end
# onegram_t<- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
# onegram<- DocumentTermMatrix(mycorpus, control=list(tokenize=onegram_t))
# onegram <- removeSparseTerms(onegram, 0.999)
# onegram_f<- sort(colSums(as.matrix(onegram)), decreasing=TRUE)
# save(onegram, file='./onegram_042616.Rdata')
## two-gram tokenizer, remove sparse terms in the end
twogram_t<- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
twogram<- DocumentTermMatrix(mycorpus, control=list(tokenize=twogram_t))
twogram<- removeSparseTerms(twogram, 0.99)
twogram_f<- sort(colSums(as.matrix(twogram)), decreasing=TRUE)
save(twogram, twogram_f, file='./twogram_042616.Rdata')
load('frequency1234.rdata')
load('threegram_042616.rdata')
threegram_f<- sort(colSums(as.matrix(threegram)), decreasing=TRUE)
save(onegram_f, twogram_f, threegram_f, fourgram_f, file='./frequency1234.Rdata')
rm('threegram')
firstname<-sapply(strsplit(names(twogram_f), ' '), function(a) a[1])
secname<-sapply(strsplit(names(twogram_f), ' '), function(a) a[2])
firsttriname<-sapply(strsplit(names(threegram_f), ' '), function(a) a[1])
sectriname<-sapply(strsplit(names(threegram_f), ' '), function(a) a[2])
thirtriname<-sapply(strsplit(names(threegram_f), ' '), function(a) a[3])
unigram<-data.frame(uni=names(onegram_f), freq=onegram_f, stringsAsFactors = F)
bigram<- data.frame(bi=names(twogram_f), freq=twogram_f, unigram=firstname, name=secname)
trigram<- data.frame(tri=names(threegram_f), freq=threegram_f, bigram=paste(firsttriname, sectriname), name=thirtriname)
save(unigram, bigram, trigram,  file='./ngram123.Rdata')
setwd("C:/Users/i55336/Downloads/Coursera/Coursera-SwiftKey/Capstone_Gabby")
library(slidify)
slidify('index.Rmd')
